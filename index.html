<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="description" content="Design & Learning Research Group Solution for euROBIN MSVC @ IROS 2024" />
  <meta name="keywords" content="Design & Learning Research Group, euROBIN, MSVC, IROS 2024" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    Transferrable Robot Skills Approaching Human-Level Versatility in Automated Task Board Manipulation - Design &
    Learning Research Group
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="css/bulma.min.css" />
  <link rel="stylesheet" href="css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="css/bulma-slider.min.css" />
  <link rel="stylesheet" href="css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="css/index.css" />
  <link rel="icon" href="./images/favicon.ico" type="image/x-icon" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WCPLFB8FXC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-WCPLFB8FXC');
  </script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3 publication-title">
              Solution for euROBIN MSVC @ IROS 2024
            </h2>
            <h1 class="title is-2 publication-title">
              Transferrable Robot Skills Approaching Human-Level Versatility in Automated Task Board Manipulation
            </h1>
            <h2 class="title is-4 publication-title-award">
              Fastest Automated Task Board Solution Award
            </h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Design and Learning Research Group (DLRG)
                <br />
                Southern Univerisity of Science and Technology
              </span>
            </div>
            <table align="center" class="table">
              <tr>
                <td>
                  <a href="https://www.sustech.edu.cn/en/">
                    <img src="./images/logo/sustech_light.png" width="300px" alt="SUSTech"></img>
                  </a>
                </td>
                <td>
                  <a href="https://bionicdl.ancorasir.com">
                    <img src="./images/logo/bionicdl.jpg" width="70px" alt="BionicDL"></img>
                  </a>
                </td>
                <td>
                  <a href="https://maindl.ancorasir.com">
                    <img src="./images/logo/maindl.jpg" width="70px" alt="MainDL"></img>
                  </a>
                </td>
              </tr>
            </table>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- ArXiv Link. -->
                <!-- <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span> -->
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://youtu.be/6FlQ3zCi53w" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ancorasir/DesignLearnRG_euROBIN"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Data Link. -->
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1yhIDuubmL7S85B2c-S1hvZppHv4G-Wow?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- zh-CN Link. -->
                <!-- <span class="link-block">
                  <a href="https://msvc-dlrg.github.io/zh-cn" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-globe"></i>
                    </span>
                    <span>中文</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="content has-text-centered">
        <img src="./images/teaser.png" alt="Teaser" width="1000px" />
      </div>
      <div class="container">
        <div id="results-carousel-hero" class="carousel results-carousel">
          <div class="item">
            <img src="./images/on-site/hero_1.jpg" alt="On-site 1" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_2.jpg" alt="On-site 2" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_3.jpg" alt="On-site 3" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_4.jpg" alt="On-site 4" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_5.jpg" alt="On-site 5" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_6.jpg" alt="On-site 6" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_7.jpg" alt="On-site 7" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_8.jpg" alt="On-site 8" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_9.jpg" alt="On-site 9" />
          </div>
          <div class="item">
            <img src="./images/on-site/hero_10.jpg" alt="On-site 10" />
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Versatility in engineering means adaptability and multi-functionality. For robotic automation, it
              signifies the ability to handle diverse tasks, easily switch between different operations, and thrive in
              changing environments. The current gap lies in developing agreed-upon frameworks and metrics that are both
              quantitative and context-appropriate, capturing not just mechanical capabilities but also cognitive
              adaptability, integration complexity, and economic value.
            </p>
            <p>
              In this paper, we present the Design and Learning Research Group's (DLRG) solution for the <a
                href="https://sites.google.com/view/eurobin-msvc">euROBIN Manipulation Skill Versatility Challenge
                (MSVC)</a> at <a href="https://iros2024-abudhabi.org">IROS 2024</a> in Abu Dhabi, UAE. The MSVC, held
              annually since 2021, is part of the euROBIN project that seeks to advance transferrable robot skills for
              the circular economy by autonomously performing tasks such as object localization, insertion, door
              operation, circuit probing, and cable management. We approached the standardized task board provided by
              event organizers that mimics industrial testing procedures by structurally decomposing the task into
              subtask skills. We created a custom dashboard with drag-and-drop code blocks to streamline development and
              adaptation, enabling rapid code refinement and task restructuring, complementing the default remote web
              platform that records the performance. Our system completed the task board in 28.2 sec in the lab (37.2
              sec on-site), nearly tripling the efficiency over the averaged best time of 83.5 sec by previous teams and
              bringing performance closer to a human baseline of 16.3 sec. By implementing subtasks as reusable code
              blocks, we facilitated the transfer of these skills to a distinct scenario, successfully removing a
              battery from a smoke detector with minimal reconfiguration.
            </p>
            <p>
              We also provide suggestions for future research and industrial practice on robotic versatility in
              manipulation automation through globalized competitions, interdisciplinary efforts, standardization
              initiatives, and iterative testing in the real world to ensure that it is measured in a meaningful,
              actionable way.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/6FlQ3zCi53w?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Robot System Design</h2>
          <div class="content has-text-justified">
            <p>
              The robot system developed by DLRG for the competition includes a robot arm (UR10e, Universal Robots), a
              camera (D435, Intel RealSense), a gripper (Hand-E, Robotiq), and a pair of 3D-printed fingertips with
              minor customization for cable handling, all mounted on the open-sourced <a
                href="https://doi.org/10.1109/AIM43001.2020.9159011">DeepClaw</a> station previously developed by the
              team for a reliable and reconfigurable implementation. The camera was mounted on a bracket between the
              gripper and the flange of the robot arm.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-full-width">
              <img src="./images/hardware/assembly.jpg" alt="Robot System Design" width="500px" />
            </div>
          </div>
          <table align="center" width="100%" border="1">
            <tr>
              <th width="20%">Equipment</th>
              <th width="30%">Image</th>
              <th width="50%">Description</th>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.universal-robots.com/products/ur10-robot">
                  UR10e Cobot
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/ur10e.png" alt="UR10e" width="60%" />
              </td>
              <td style="vertical-align: middle">
                The UR10e cobot is mounted on the DeepClaw station and controlled through the real-time data exchange
                (RTDE) interface by the laptop.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://robotiq.com/products/adaptive-grippers#Hand-E">
                  Robotiq Hand-E gripper
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/hand-e.png" alt="Hand-E" width="40%" />
              </td>
              <td style="vertical-align: middle">
                The Hand-E gripper is mounted on the tool flange of the UR10e cobot and controlled via the Modbus RTU
                RS485 protocol.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://cad.onshape.com/documents/43edc50e275c72eace7a4839">
                  3D-printed fingertips
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/fingertip.png" alt="3D-printed fingertips" width="70%" />
              </td>
              <td style="vertical-align: middle">
                The fingertips are re-designed based on the original fingertips of the Robotiq Hand-E gripper to adapt
                to the competition tasks by adding grooves. They are fabricated using nylon (PA12, HP) through multi-jet
                fusion (MJF)
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.intelrealsense.com/depth-camera-d435i">
                  Intel Realsense D435i camera
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/d435i.png" alt="Realsense D435i" width="80%" />
              </td>
              <td style="vertical-align: middle">
                The Realsense D435i RGB-D camera is mounted on the UR10e cobot
                through a bracket and connected to the laptop through a USB
                cable.
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://cad.onshape.com/documents/01d4267b0af8aab9d6acb1ab">
                  Camera bracket
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/camera_bracket.png" alt="Camera bracket" width="80%" />
              </td>
              <td style="vertical-align: middle">
                The camera bracket is designed to mount the Realsense D435i
                RGB-D camera on the UR10e cobot. It is frabricated by CNC with
                Al6061.
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.zhiyun-tech.com/en/product/detail/867">
                  ZHIYUN FIVERAY M20 fill light
                </a>
                (Optinal)
              </td>
              <td style="vertical-align: middle">
                <img src="./images/hardware/fiveray-m20.png" alt="FIVERAY M20" width="70%" />
              </td>
              <td style="vertical-align: middle">
                The ZHIYUN FIVERAY M20 fill light is mounted on the camera to
                provide sufficient illumination. It is up to 2010 Lux (0.5m)
                and able to work for more than 40 minutes without charging.
              </td>
            </tr>
          </table>
          <div class="content has-text-justified">
            <br />
            <p>
              We also designed a web-based user interface. On the left side of the interface was a <i>blueprint</i>
              panel outlining the robot arm's joints and links, the camera, and the gripper with fingertips. The middle
              includes a <i>3D scene</i> of the robotic arm showing its current state in Cartesian space, a
              <i>camera</i> view of the live video stream taken from the wrist overlayed with key feature recognition
              status, and a stack of <i>data curve</i> windows showing the robot's angles and velocities in the joint
              space. The right is a <i>command</i> panel with code blocks for each subtask skill and a drag-and-drop
              interface to restructure and refine the overall task flow rapidly. We also added an extra record button to
              start and stop recording robot data for convenient execution and redeployment.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-full-width">
              <img src="./images/web_ui.jpg" alt="Web-based User Interface" width="800px" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Solution for SubTasks</h2>
          <h3 class="title is-4">SubTask 1: Find Board & Press Button</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random position for each trial. Accurately locating the task board is
              crucial for successful task execution. A fine-tuned <a
                href="https://github.com/ultralytics/ultralytics">YOLOv8</a> model was used to detect the red button,
              the blue button and the red test port. As the positional relationship between these task board elements
              was known and fixed,the global 6D pose of the board was estimated relative to the robot frame.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-half">
              <img src="./images/yolo_pose.jpg" alt="Task Board Pose Estimation" width="100%" />
            </div>
            <div class="column is-half">
              <img src="./images/ref_frame.jpg" alt="Task Board Pose Relative to Robot Base" width="100%" />
            </div>
          </div>
          <div class="content has-text-justified">
            <p>
              To press the blue button, the gripper first moved above it and downward. Then
              the gripper moved upward, preparing for the next task.
            </p>
          </div>
          <div class="content">
            <img src="./images/blue_button.jpg" alt="Press Blue Button" width="100%" />
          </div>
          <h3 class="title is-4">
            SubTask 2: Move Slider to Setpoints on Screen
          </h3>
          <div class="content has-text-justified">
            <p>
              After the gripper approached to and securely gripped the slider, the screen was detected using another
              fine-tuned YOLOv8 model. The red pointer refered to the current position, the yellow one refered to the
              first target position, and the green one refered to the second target position. The pixels of the pointers
              displayed on the screen were extracted based on their colors to calculate the pixel distance error, which
              was then projected to the actual distance relative to the robot frame, guiding the gripper to move the
              slider to the target.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-half">
              <img src="./images/slider_screen.jpg" alt="Screen Detection" width="700px" />
            </div>
            <div class="column is-half">
              <video id="teaser" autoplay muted loop controls width="700px">
                <source src="./videos/move_slider.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <h3 class="title is-4">SubTask 3: Plug in Probe into Test Port</h3>
          <div class="content has-text-justified">
            <p>
              The gripper first moved above the probe and downward to grip it. Then the probe was plugged into the red
              test port. To increase the determinacy of wrapping the cable in subtask 5, the probe was rotated to a
              certain angle.
            </p>
          </div>
          <div class="content">
            <img src="./images/plug_in_probe.jpg" alt="Plug in Probe into Test Port" width="100%" />
          </div>
          <h3 class="title is-4">SubTask 4: Open Door & Probe Circuit</h3>
          <div class="content has-text-justified">
            <p>
              To speed up the task execution, the gripper pulled out the probe and pried up the door with the probe tip,
              instead of gripping the door. Then the probe was inserted into the testing slot and replaced. The
              challenge of this subtask is that the pose fo the probe might be changed due to the contact with the task
              board, but the grooves on the fingertips helped the gripper to securely grasp the probe, increasing the
              accuracy and robustness of the whole process.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/probe_circuit.mp4" type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4">SubTask 5: Wrap Cable & Replace Probe</h3>
          <div class="content has-text-justified">
            <p>
              To wrap the cable, we collect the gripper poses from human
              demonstrated motions, and transform the poses to the task board
              frame, making it invariant to the absolute position and
              orientation of the board.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/wrap_cable.mp4" type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4">SubTask 6: Press Stop Trial Button</h3>
          <div class="content has-text-justified">
            <p>
              Pressing the stop trial button (red button) is similar as previous pressing the blue button. After the
              gripper pressed the button, the task board stopped time recording and the trial was finished.
            </p>
          </div>
          <div class="content">
            <img src="./images/stop_trial_button.jpg" alt="Press Stop Trial Button" width="100%" />
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Robust Consecutive Trials</h2>
          <div class="content has-text-justified">
            <p>
              Following the competition protocol, we performed five consecutive trials at 1 m/s, randomly repositioning
              the task board to test robustness.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/consecutive_trials.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">High-speed Trial</h2>
          <div class="content has-text-justified">
            <p>
              We then increased speed to 2.5 m/s for a successful high-speed trial completed in 28.2 sec, nearly
              tripling the efficiency over the averaged best time of 83.5 sec by previous teams and bringing performance
              closer to a human baseline of 16.3 sec.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/fastest_trial.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">On-site Demonstration</h2>
          <div class="content has-text-justified">
            <p>
              During the on-site competition at IROS 2024 in Abu Dhabi, our system completed the task board in 37.2 sec,
              winning the Fastest Automated Task Board Solution Award.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/on_site_trial.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Skill Transferability</h2>
          <div class="content has-text-justified">
            <p>
              The implemented task board skills, such as grasping, dragging, inserting, and pressing, readily transfer
              to broader applications, such as replacing a smoke detector's battery, which is part of the competition
              requirement. We decomposed this new scenario into five subtasks directly mapped from previous skills with
              adjusted TCP trajectories and gripper angles.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop controls width="700px">
              <source src="./videos/battery_replacement.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Discussion</h2>
          <div class="content has-text-justified">
            <p>
              We compared the performance of our system with human and seven other teams (five best teams in
              2023, and two teams in 2024). The result indicates that our system had a significant
              advantage in executing speed compared to other teams in all subtasks, especially ST1, ST4, and ST5, which
              are close to the human level.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-half">
              <img src="./images/time_comparison.jpg" alt="Time Comparison" width="100%" />
            </div>
            <div class="column is-half">
              <img src="./images/team_solution.jpg" alt="Team Solution" width="100%" />
            </div>
          </div>
          <div class="content has-text-justified">
            <p>
              While preparing for the competition, despite the availability of more advanced tools such as <a
                href="https://robot-spns.github.io/">soft robotic fingers</a> with omni-directional adaption and
              state-of-the-art tactile sensing, the team still chose a more structured and industrial solution where the
              efficiency of task completion is more emphasized over intelligence integration or exploration. The
              outcomes of such competitions seem to reveal a subtle trend: as tasks become more structured and
              performance-oriented, there is a tendency for solutions to converge on efficient but relatively inflexible
              methods. Similar observations can be found in global competitions like the <a
                href="https://doi.org/10.1109/TASE.2016.2600527">Amazon Picking Challenge</a>. To counter this trend,
              future competitions or industrial testbeds should introduce greater complexity and variability through
              diverse objects, dynamic environments, and evolving subtasks to encourage the use of advanced perception,
              learning algorithms, and adaptive end-effectors. Such challenges will drive the development of intuitive
              interfaces and autonomous reconfiguration, bridging the gap between human and robot adaptability.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section is-small">
    <div class="container is-max-desktop">
      <div class="container has-text-centered">
        <h2 class="title is-3 is-size-4-mobile">Our Team</h2>
        <div class="publication-authors is-flex justify-content is-justify-content-space-around is-hidden-mobile">
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://hanxudong.cc">
                <img class="is-rounded" src="./images/team/xudong.jpg" alt="Xudong Han">
              </a>
            </figure>
            <a href="https://hanxudong.cc">Xudong Han</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://github.com/AlanSunHR">
                <img class="is-rounded" src="./images/team/haoran.jpg" alt="Haoran Sun">
              </a>
            </figure>
            <a href="https://github.com/AlanSunHR">Haoran Sun</a><sup>1, 2</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://gabriel-ning.github.io">
                <img class="is-rounded" src="./images/team/ning.jpg" alt="Ning Guo">
              </a>
            </figure>
            <a href="https://gabriel-ning.github.io">Ning Guo</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://geshengpsn.github.io/">
                <img class="is-rounded" src="./images/team/sheng.jpg" alt="Sheng Ge">
              </a>
            </figure>
            <a href="https://geshengpsn.github.io/">Sheng Ge</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://maindl.ancorasir.com">
                <img class="is-rounded" src="./images/team/fang.jpg" alt="Fang Wan">
              </a>
            </figure>
            <a href="https://maindl.ancorasir.com">Fang Wan</a><sup>1, </sup>*
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-128x128">
              <a href="https://bionicdl.ancorasir.com">
                <img class="is-rounded" src="./images/team/chaoyang.jpg" alt="Chaoyang Song">
              </a>
            </figure>
            <a href="https://bionicdl.ancorasir.com">Chaoyang Song</a><sup>1, </sup>*
          </div>
        </div>

        <div class="is-flex is-flex-direction-row is-flex-wrap-wrap is-justify-content-space-evenly is-hidden-tablet">
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://hanxudong.cc">
                <img class="is-rounded" src="./images/team/xudong.jpg" alt="Xudong Han">
              </a>
            </figure>
            <a href="https://hanxudong.cc">Xudong Han</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://github.com/AlanSunHR">
                <img class="is-rounded" src="./images/team/haoran.jpg" alt="Haoran Sun">
              </a>
            </figure>
            <a href="https://github.com/AlanSunHR">Haoran Sun</a><sup>1, 2</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://gabriel-ning.github.io">
                <img class="is-rounded" src="./images/team/ning.jpg" alt="Ning Guo">
              </a>
            </figure>
            <a href="https://gabriel-ning.github.io">Ning Guo</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://geshengpsn.github.io/">
                <img class="is-rounded" src="./images/team/sheng.jpg" alt="Sheng Ge">
              </a>
            </figure>
            <a href="https://geshengpsn.github.io/">Sheng Ge</a><sup>1</sup>
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://maindl.ancorasir.com">
                <img class="is-rounded" src="./images/team/fang.jpg" alt="Fang Wan">
              </a>
            </figure>
            <a href="https://maindl.ancorasir.com">Fang Wan</a><sup>1,</sup>*
          </div>
          <div class="author-block has-text-centered has-addons-centered">
            <figure class="image is-96x96">
              <a href="https://bionicdl.ancorasir.com">
                <img class="is-rounded" src="./images/team/chaoyang.jpg" alt="Chaoyang Song">
              </a>
            </figure>
            <a href="https://bionicdl.ancorasir.com">Chaoyang Song</a><sup>1,</sup>*
          </div>
        </div>
        <div class="publication-authors">
          <span class="author-block"><sup>1</sup>Southern University of Science and Technology, </span>
          <span class="author-block"><sup>2</sup>University of Hongkong, </span>
          <span class="author-block">*Corresponding Author</span>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template credit to
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, and is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
                Attribution-ShareAlike 4.0 International
                License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script>
    bulmaCarousel.attach('#results-carousel-hero', {
      slidesToScroll: 1,
      slidesToShow: 3,
      autoplay: true,
      loop: true,
      infinite: true,
      duration: 500,
      autoplaySpeed: 5000,
    });

    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
</body>

</html>