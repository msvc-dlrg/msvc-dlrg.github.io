<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Design and Learning Research Group Submission - euROBIN MSVC @ IROS 2024</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="css/bulma.min.css">
  <link rel="stylesheet" href="css/bulma-carousel.min.css">
  <link rel="stylesheet" href="css/bulma-slider.min.css">
  <link rel="stylesheet" href="css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="css/index.css">
  <link rel="icon" href="images/icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3 publication-title">euROBIN MSVC @ IROS 2024</h2>
            <h1 class="title is-2 publication-title">Design and Learning Research Group Submission</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Chaoyang Song,
              </span>
              <span class="author-block">
                Ning Guo,
              </span>
              <span class="author-block">
                Haoran Sun,
              </span>
              <span class="author-block">
                Xudong Han
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://bionicdl.ancorasir.com">Design and Learning Research Group</a>
                @ <a href="https://www.sustech.edu.cn/en">SUSTech</a>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ancorasir/DesignLearnRG_euROBIN"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="images/teaser.png" alt="Teaser" width="100%">
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              We present the first method capable of photorealistically reconstructing a non-rigidly
              deforming scene using photos/videos captured casually from mobile phones.
            </p>
            <p>
              Our approach augments neural radiance fields
              (NeRF) by optimizing an
              additional continuous volumetric deformation field that warps each observed point into a
              canonical 5D NeRF.
              We observe that these NeRF-like deformation fields are prone to local minima, and
              propose a coarse-to-fine optimization method for coordinate-based models that allows for
              more robust optimization.
              By adapting principles from geometry processing and physical simulation to NeRF-like
              models, we propose an elastic regularization of the deformation field that further
              improves robustness.
            </p>
            <p>
              We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
              photos/videos into deformable NeRF
              models that allow for photorealistic renderings of the subject from arbitrary
              viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
              using a
              rig with two mobile phones that take time-synchronized photos, yielding train/validation
              images of the same pose at different viewpoints. We show that our method faithfully
              reconstructs non-rigidly deforming scenes and reproduces unseen views with high
              fidelity.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Robot Platform</h2>
          <div class="content has-text-justified">
            <p>
              The robot system used in the competition consists of a UR10e collebrative robot, a 
              Robotiq Hand-E gripper, a pair of 3D-printed fingertips, a Realsense D435i RGB-D 
              camera, a camera bracket, and a ZHIYUN FIVERAY M20 fill light.
            </p>
          </div>
          <table align="center" width="80%" border="1">
            <tr>
              <th width="20%">Equipment</th>
              <th width="30%">Image</th>
              <th width="50%">Description</th>
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                <a href="https://www.universal-robots.com/products/ur10-robot">
                  UR10e Cobot
                </a>
              </td>
              <td style="vertical-align: middle;">
                <img src="images/ur10e.png" alt="UR10e" width="60%">
              </td>
              <td style="vertical-align: middle;">
                The UR10e cobot is mounted with the table and controlled through the real-time 
                data exchange (RTDE) interface by the laptop.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                <a href="https://robotiq.com/products/adaptive-grippers#Hand-E">
                    Robotiq Hand-E gripper
                </a>
            </td>
              <td style="vertical-align: middle;">
                <img src="images/hand-e.png" alt="Hand-E" width="40%">
              </td>
              <td style="vertical-align: middle;">
                The Hand-E gripper is mounted on the tool flange of the UR10e cobot and controlled
                via the UR10e.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                3D-printed fingertips
            </td>
              <td style="vertical-align: middle;">
                <img src="images/fingertip.png" alt="3D-printed fingertips" width="70%">
              </td>
              <td style="vertical-align: middle;">
                The fingertips are re-designed based on the original fingertips of the
                Robotiq Hand-E gripper to adapt to the competition tasks by adding grooves. They
                are 3D-printed with nylon (HP PA12).
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                <a href="https://www.intelrealsense.com/depth-camera-d435i">
                    Intel Realsense D435i camera
                </a>
            </td>
              <td style="vertical-align: middle;">
                <img src="images/d435i.png" alt="Realsense D435i" width="80%">
              </td>
              <td style="vertical-align: middle;">
                The Realsense D435i RGB-D camera is mounted on the UR10e cobot through a bracket
                and connected to the laptop through a USB cable.
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                Camera bracket
            </td>
              <td style="vertical-align: middle;">
                <img src="images/camera_bracket.png" alt="Camera bracket" width="80%">
              </td>
              <td style="vertical-align: middle;">
                The camera bracket is designed to mount the Realsense D435i RGB-D camera on the UR10e
                cobot. It is frabricated by CNC with Al6061.
            </tr>
            <tr>
              <td style="vertical-align: middle;">
                <a href="https://www.zhiyun-tech.com/en/product/detail/867">
                    ZHIYUN FIVERAY M20 fill light
                </a>
              </td>
              <td style="vertical-align: middle;">
                <img src="images/fiveray-m20.png" alt="FIVERAY M20" width="70%">
              </td>
              <td style="vertical-align: middle;">
                The ZHIYUN FIVERAY M20 fill light is mounted on the camera to provide sufficient
                illumination. It is up to 2010 Lux (0.5m) and able to work for more than 40 minutes 
                without charging.
              </td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Software</h2>
          <div class="content has-text-justified">
            <p>
              The robot system used in the competition consists of a UR10e collebrative robot, a 
              Robotiq Hand-E gripper, a pair of 3D-printed fingertips, a Realsense D435i RGB-D 
              camera, a camera bracket, and a ZHIYUN FIVERAY M20 fill light.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Solution</h2>
          <h3 class="title is-4">Task 0: Press Start Trial Button</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 1: Fing Board & Press Button</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 2: Move Slider to Setpoints on Screen</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 3: Plug in Probe into Test Port</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 4: Open Door, Probe Circuit</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 5: Wrap Cable, Replace Probe</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
          <h3 class="title is-4">Task 6: Press Stop Trial Button</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro strips to hold it in 
              place for a trial. Therefore, for all tasks (including this task), the task board 
              needs to be localized.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Discussion</h2>
          <div class="content has-text-justified">
            <p>
              The robot system used in the competition consists of a UR10e collebrative robot, a 
              Robotiq Hand-E gripper, a pair of 3D-printed fingertips, a Realsense D435i RGB-D 
              camera, a camera bracket, and a ZHIYUN FIVERAY M20 fill light.
            </p>
          </div>
        </div>
      </div>
    </div>

  <footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        Website template credit to 
                        <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, 
                        and is licensed under a 
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
  </footer>
</body>
</html>
